{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Copyright 2020 Vasile Rus, Andrew M. Olney and made available under [CC BY-SA](https://creativecommons.org/licenses/by-sa/4.0) for text and [Apache-2.0](http://www.apache.org/licenses/LICENSE-2.0) for code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naïve Bayes\n",
    "\n",
    "Naïve Bayes is a supervised data science method typically used for **classification/categorization** tasks as exemplified before in, for instance, the logistic regression notebook.\n",
    "For that reason, it can be viewed as estimating the probabilities of a number of outcome variable values, e.g., the probabilities of categories in classification.\n",
    "To classify a particular object or instance $X$, the class with the highest probability among all possible classes $1$ to $C$ is taken as shown below:\n",
    "\n",
    "$$class (X) = argmax_{c \\in (1..C)} P(c_i|X)$$ \n",
    "        \n",
    "While quite successful in classification tasks, the actual estimated probabilities for each class are not very reliable.\n",
    "\n",
    "In this notebook, we focus on multinomial, hard classification tasks.\n",
    "\n",
    "### What you will learn\n",
    "\n",
    "In this notebook, you will learn about naïve Bayes, an original data science paradigm to approach primarily classification tasks, and how it can be used to infer from labeled/annotated naïve Bayes based classifiers.  \n",
    "We will study the following:\n",
    "\n",
    "- The basics of naïve Bayes\n",
    "- The meaning of “naïve” in naïve Bayes\n",
    "- Details about how naïve Bayes models are trained\n",
    "- Evaluation of performance for naïve Bayes classifiers\n",
    "\n",
    "\n",
    "### When to use naïve Bayes\n",
    "\n",
    "Naïve Bayes classifiers are useful when you have a categorical response/outcome variable and there are multiple features/predictors that can be used to predict the correct value of the outcome variable. \n",
    "The ultimate goal is to build automatically a probabilistic model to predict the correct value of the outcome variable for a new instance described by the set of predictors/features. \n",
    "Naïve Bayes outputs a probability distribution over the values of the outcome variable and therefore for each class a probability value is being generated. \n",
    "The category with the highest probability is typically chosen as the correct/most-likely category for the corresponding instance. \n",
    "Naïve Bayes has the advantage of being simple and highly accurate for classification when features can be treated independently, comparable to logistic regression but more easily extended to many categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Foundations of Naïve Bayes for Binary, Hard Classification\n",
    "\n",
    "We briefly review in this section the mathematical formulation of the naïve Bayes method for multinomial, hard classification problems. \n",
    "That is, we assume the outcome for one instance or object can be one and only one category out $C$ possible categories.\n",
    "\n",
    "The naïve Bayes method relies on Bayes' Theorem shown below:\n",
    "\n",
    "$$P (Y|X) = \\frac{P(Y)P(X|Y)}{P(X)}$$\n",
    "\n",
    "The term $P (Y|X) $ is called the posterior, the term $P(Y)$ is called the prior, and the term $P(X|Y)$ is called the likelihood.\n",
    "\n",
    "In a classification case, Y can take as value any of the classes $c \\in (1..C)$ and X is described as a set of features/predictors $X=(x_1,..,x_P)$. \n",
    "Then Bayes' Theorem becomes:\n",
    "\n",
    "$$P (Y=c_i| (x_1,..,x_P)) = \\frac{P(Y=c_i)P(x_1,..,x_P|Y=c_i)}{P(x_1,..,x_P)}$$\n",
    "\n",
    "The naïve Bayes method takes this theorem and based on the naive assumption of the predictors $x_i$ being independent, i.e., meaning $P(x_1,..,x_P|Y=c_i)$ is approximated by $\\prod \\limits _{j=1} ^P P(x_j|c_i)$, it re-writes the theorem in the following form:\n",
    "\n",
    "$$P (Y=c_i| (x_1,..,x_P)) = \\frac{P(Y=c_i) \\prod \\limits _{j=1} ^P P(x_j|c_i)}{P(x_1,..,x_P)}$$\n",
    "\n",
    "This naive formulation of the theorem is more manageable in terms of estimating the parameters of the distributions involved and in particular of the likelihood probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Naïve Bayes Classifier\n",
    "\n",
    "Training a naïve Bayes classifier implies deriving the prior and likelihood distributions from training data based on the naive formulation of Bayes' Theorem.\n",
    "\n",
    "The prior $P(c_i)$ is derived using the following expression:\n",
    "\n",
    "$$ P(c_i)= \\frac{{\\#} c_i}{N}$$\n",
    "\n",
    "where ${\\#} c_i$ is the number of training instances labeled with class $i$ and $N$ is the total number of training instances.\n",
    "\n",
    "The likelihood $P (X | Y) = \\prod _{j=1} ^P P(x_j|c_i) = \\prod P(x_1|c_i)P(x_2|c_i)... P(x_P|c_i)$ is derived by multiplying individual conditional distributions for each predictor $x_i$ as shown below:\n",
    "\n",
    "$$ P(x_i|c_i) = \\frac{{\\#} x_{ci}}{{\\#} c_i}$$\n",
    "\n",
    "Once the prior and likelihood distributions derived, to predict the most likely class for a new instance $X=(x_i, ..., x_P)$ we apply the naïve Bayes formula:\n",
    "\n",
    "$$class (X) = argmax_{c \\in (1..C)} {P(c_i|X)} = argmax_{c \\in (1..C)} P (Y=c_i| (x_1,..,x_P)) = argmax_{c \\in (1..C)} \\frac{P(Y=c_i) P(x_1|c_i)P(x_2|c_i)... P(x_P|c_i)}{P(x_1,..,x_P)} $$\n",
    "\n",
    "Since the denominator does not depend on $c_i$, the argument of argmax, we can ignore the denominator.\n",
    "Then the most likely class can be simply obtained using this formula:\n",
    "\n",
    "$$class (X) = argmax_{c \\in (1..C)} P(c_i|X) = argmax_{c \\in (1..C)} P(Y=c_i) P(x_1|c_i) P(x_2|c_i) ... P(x_P|c_i)$$ \n",
    "\n",
    "That is, the most likely class is the class correspond to the posterior probability estimated based on the above naive formulation of the Bayes Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- NOTE: this has already been covered -->\n",
    "<!-- ## Peformance Evaluation for Classification Methods including Naïve Bayes\n",
    "\n",
    "The typical performance metrics for classifiers are accuracy, precision, and recall. These are typical derived by compared the predicted output to the golden or actual output/categories in the expert labelled dataset.\n",
    "\n",
    "For a binary classification case, we denote the category 1 as the positive category and category 0 as the negative category. Using this new terminology, When comparing the predicted categories to the actual categories we may end up with the following cases:\n",
    "* True Positives (TP): instances predicted as belonging to the positive category and which in fact do belong to the positive category\n",
    "* True Negatives (TN): instances predicted as belonging to the negative category and which in fact do belong to the negative category\n",
    "* False Positives (FP): instances predicted as belonging to the positive category and which in fact do belong to the negative category\n",
    "* False Negatives (FN): instances predicted as belonging to the negative category and which in fact do belong to the positive category\n",
    "\n",
    "From these categories, we define the following metrics:\n",
    "\n",
    "$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\n",
    "$Precision = \\frac{TP}{TP + FP}$\n",
    "\n",
    "$Recall = \\frac{TP}{TP + FN}$\n",
    "\n",
    "Classfication methods that have a high accuracy are preferred in general although in some case maximizing precision or recall may be preferred. For instance, a high recall is highly recommended when making medical diagnosis since it is preferrable to err on mis-diagnosing someone as having cancer as opposed to missing someone who indeed has cancer, i.e., the method should try not to miss anyone who may indeed have cancer. \n",
    "\n",
    "In general, there is a trade-off between precision and recall. If precision is high then recall is low and viceversa. Total recall (100% recall) is achievable by always predicting the positive class, i.e., label all instances as positive, in which case precision will be very low. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Naïve Bayes\n",
    "\n",
    "The data we will use is the `nursery` dataset, which ranks applications for nursery schools in Slovenia during the 1980s.\n",
    "Because the original dataset is a fair bit larger, we've randomly sampled 2000 rows.\n",
    "\n",
    "The goal is to predict `rank`.\n",
    "\n",
    "| Variable | Type    | Description                                        |\n",
    "|:----------|:---------|:----------------------------------------------------|\n",
    "| parents  | Nominal | usual, pretentious, great_pret                     |\n",
    "| has_nurs | Nominal | proper, less_proper, improper, critical, very_crit |\n",
    "| form     | Nominal | complete, completed, incomplete, foster            |\n",
    "| children | Nominal | 1, 2, 3, more                                      |\n",
    "| housing  | Nominal | convenient, less_conv, critical                    |\n",
    "| finance  | Nominal | convenient, inconv                                 |\n",
    "| social   | Nominal | non-prob, slightly_prob, problematic               |\n",
    "| health   | Nominal | recommended, priority, not_recom                   |\n",
    "| rank    | Nominal | not_recom, recommend, very_recom, priority, spec_prior   |\n",
    "\n",
    "<div style=\"text-align:center;font-size: smaller\">\n",
    " <b>Source:</b> This dataset was taken from the <a href=\"https://archive.ics.uci.edu/ml/datasets/Nursery\">UCI Machine Learning Repository library\n",
    "    </a></div>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Import `pandas` so we can load a dataframe:\n",
    "\n",
    "- `import pandas as pd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataframe:\n",
    "\n",
    "- Create `dataframe` and set it to `with pd do read_csv using` a list containing\n",
    "    - `\"datasets/nursery.csv\"`\n",
    "- `dataframe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data\n",
    "\n",
    "Let's check the data makes sense with the five figure summary:\n",
    "\n",
    "- `with dataframe do describe using` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did we get?\n",
    "It's not a five figure summary because our variables are nominal, and there's no such thing as mean, median, etc., with nominal data.\n",
    "Instead we have the number of *unique* levels of each variable, the *top* or most frequent level, and the *freq*uency of that level.\n",
    "Moreover, the count is 2000 across all variables, indicating there are no NaNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the variables are nominal, many of our standard tools won't work.\n",
    "For example, we can't use a correlation matrix/heatmap, because correlation isn't defined for nominal variables.\n",
    "There is something called [Cramer's V](https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9) that is close, but it requires some custom coding that's a bit out of scope for us.\n",
    "One thing we can do is plot each variable separately.\n",
    "\n",
    "First import `plotly.express` to do plots:\n",
    "\n",
    "- `import plotly.express as px`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next create an empty histogram figure:\n",
    "\n",
    "- Create `fig` and set it to `with px do histogram using`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create all the figures in a loop:\n",
    "\n",
    "- `for i in from dataframe get columns`\n",
    "    - Set `fig` to `with px do histogram using` a list containing\n",
    "        - `dataframe`\n",
    "        - freestyle `x=i` (`i` will take on the name of a column on each loop)\n",
    "    - Empty freestyle followed by `with fig do show using`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, all the variables have levels that occur with about the same frequency, except for `rank`, which is very imbalanced with respect to `very_recom`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train/test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's separate our predictors (`X`) from our class label (`Y`), putting each into its own dataframe:\n",
    "\n",
    "- Create `X` and set to `with dataframe do drop using` a list containing\n",
    "    - freestyle `columns=[\"rank\"]`\n",
    "- Create `Y` and set to `dataframe [ ]` containing a list with `\"rank\"` inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we will use is Bernoulli naive Bayes.\n",
    "This model needs numeric predictors, but can have nominal class labels.\n",
    "So we need to get dummies for `X` only:\n",
    "\n",
    "- Set `X` to `with pd get_dummies using` a list containing\n",
    "    - `X`\n",
    "- `X` (so you can see what happened)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to split the data into train/test sets, which requires `sklearn.model_selection`:\n",
    "\n",
    "- `import sklearn.model_selection as model_selection`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And do the actual split:\n",
    "\n",
    "- Create `splits` and set to `with model_selection do train_test_split` using a list containing\n",
    "    - `X`\n",
    "    - `Y`\n",
    "    - freestyle `random_state=1`\n",
    "    \n",
    "Setting random_state will make our results match each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model\n",
    "\n",
    "We need to import libraries for:\n",
    "\n",
    "- Naïve Bayes\n",
    "- Metrics\n",
    "- Ravel\n",
    "\n",
    "So do the following imports:\n",
    "\n",
    "- `import sklearn.naive_bayes as naive_bayes`\n",
    "- `import sklearn.metrics as metrics`\n",
    "- `import numpy as np`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the naive Bayes model:\n",
    "\n",
    "- Create variable `naiveBayes` and set it to `with naive_bayes create BernoulliNB using` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model by calling `fit` on it:\n",
    "\n",
    "-  `with naiveBayes do fit using` a list containing\n",
    "    - `in list splits get # 1` (this is Xtrain)\n",
    "    - `with np do ravel using` a list containing\n",
    "        - `in list splits get # 3` (this is Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, get predictions:\n",
    "\n",
    "- Create `predictions`\n",
    "- Set it to `with naiveBayes do predict using` a list containing\n",
    "    - `in list splits get #2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the accuracy:\n",
    "\n",
    "- `with metrics do accuracy_score using` a list containing\n",
    "    - `in list splits get # 4`  (this is `Ytest`)\n",
    "    - `predictions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And get the recall and precision:\n",
    "\n",
    "- `print with metrics do classification_report using` a list containing\n",
    "    - `in list splits get # 4`  (this is `Ytest`)\n",
    "    - `predictions`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance is surprisingly good except for the infrequent class, `very_recom`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing\n",
    "\n",
    "### Feature importance\n",
    "\n",
    "Extracting feature importance from naive Bayes models in `sklearn` is a bit more work than for other models.\n",
    "\n",
    "We need to create a dataframe of the probabilities of predictors given the class label, i.e. the likelihoods, then give that dataframe correct row/column names, and finally raise it to the power of ten (because the default output is log):\n",
    "\n",
    "- Create variable `output`\n",
    "- Set it to `with pd create DataFrame` using a list containing\n",
    "    - freestyle `naiveBayes.feature_log_prob_`\n",
    "- Freestyle `output.index = naiveBayes.classes_`\n",
    "- Freestyle `output.columns = X.columns`\n",
    "- Freestyle `output = 10 ** output`\n",
    "- `output` (to display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column in this table shows the probability of that column's predictor given the classes shown in each row.\n",
    "So `parents_great_pret` makes `spec_prior` .23 likely, the most likely class for this predictor.\n",
    "\n",
    "It's a bit hard to read these, so we can also plot them in a loop:\n",
    "\n",
    "- `for i in from output get columns`\n",
    "    - Set `fig` to `with px do bar using` a list containing\n",
    "        - `output`\n",
    "        - freestyle `x=i` (`i` will take on the name of a column on each loop)\n",
    "    - Empty freestyle followed by `with fig do show using`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite a few of these predictors make one class label much more likely than the other labels.\n",
    "This suggests that the naive Bayes assumption of independent predictors is reasonable for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xpython",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
